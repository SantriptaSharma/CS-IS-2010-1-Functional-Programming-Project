\documentclass[12pt, titlepage]{article}
\usepackage[a4paper, total={6.5in, 10in}]{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}

\usepackage{tocloft}

\cftsetindents{section}{0em}{2em}
\cftsetindents{subsection}{0em}{2em}

\renewcommand\cfttoctitlefont{\hfill\Large\bfseries}
\renewcommand\cftaftertoctitle{\hfill\mbox{}\vspace*{6mm}}

\setcounter{tocdepth}{2}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Haskell,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{braket}
\usepackage{qcircuit}
\usepackage{graphicx}

\newcommand{\code}{CS-IS-2010-1}
\newcommand{\name}{Functional Programming Report}
\newcommand{\me}{Santripta Sharma}

\newcommand{\prob}[1]{\mathbb{P}\left[#1\right]}

\newcommand{\kz}{\ket{0}}
\newcommand{\ko}{\ket{1}}
\newcommand{\kp}{\ket{\phi}}
\newcommand{\ks}{\ket{\psi}}
\newcommand{\kso}{\ket{\psi_1}}
\newcommand{\kst}{\ket{\psi_2}}

\newcommand{\rec}[1]{\frac{1}{#1}}

\newtheorem{lemma}{Lemma}

\newenvironment*{qparts}{\begin{enumerate}[label=(\alph*)]}{\end{enumerate}}

\title{Inference \& Evaluation Engine for Feed-Forward Neural Networks in Haskell\vspace*{2mm}

{\large \code: \name}

{\large Supervisor: \href{https://www.ashoka.edu.in/profile/partha-pratim-das/}{Partha Pratim Das}}}
\author{\textbf{\me}\\(1020211136)}
\date{\today}

\markright{\code\hspace{0.5in}\name\hspace{0.5in}\me}

\parindent=0pt

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Introduction}
Through the course of this project, we have developed a command-line tool to run inference on (or evaluate) an arbitrary feed-forward neural network (for classification tasks). The primary goal for the project was to gain familiarity with haskell (and the functional programming paradigm) by implementing a medium-sized project using it.\bigskip

In this regard, the project has been quite succesful, forcing me to cover a lot of the bases in a relatively short time. The entire process took me from the basics, performing command-line I/O, and using the haskell tool stack, all the way to writing parser combinators for binary data, using monads, and implementing test suites with HSpec.\bigskip

Besides the learning outcomes from the project, the tool that has been developed is also useful in practical settings. It has been designed \& developed keeping in mind optimality, generalizability, and usability. An example use case of this tool is one where the user has been handed a trained classifier network (by, say, a tech-consultancy firm), and they wish to verify its performance on their data, without writing an evaluation/inference script on their own (due to time constraints or inexperience with the technology).

\section{Background \& Motivation}
\subsection{Motivation}
Today, Machine Learning has left no domain untouched. What initially started as a niche subfield of statistics has now found its application in nearly every sector \& discipline. This poses a simple problem: A large chunk of the users of these models are not familiar with how they work or how to use them. This has led to widespread black-boxing, where the development/training of the model is often outsourced to someone with this technical knowledge, with the 'client' being given instructions on how to use the model.\bigskip

Besides the inherent security risk, where the external party could place a backdoor into the model, this approach also forces the client to take the external party at their word when it comes to the effectiveness of the model, at least initially, which opens the door up to being defrauded. Additionally, in some cases, the instructions provided may be obtuse, rendering the client unable to use the model.\bigskip

Therefore, there is a need to be served for providing a solution that can perform inference and evaluation on these models, while also being friendly to users who aren't very tech-savvy. In this project, we particularly focus on a popular subclass of ML models, the feed-forward classifier, which is a generalisation of linear/logistic regression, and a foundational piece in most deep neural networks, with this unique position leading it to be one of the most common architectures for these models.

\subsection{Background: Feed-Forward Networks} \label{sec:2.2}
\begin{figure}
	\includegraphics[width=0.8\pdfpagewidth]{../images/FFN-structure.png}
	\caption{\label{fig:1}a generic FFN with $n_{hidden} = 2$}
\end{figure}
The Feed-Forward Network (FFN) is a versatile network architecture for deep neural networks, able to be used as both classifiers or regressors (though this project restricts its scope to the former). It is comprised of a set of layers ($L = \{L_1, L_2, \dots, L_{n_{hidden} + 2}\}$), each containing nodes, called 'neurons', each of which are connected to every neuron in the next layer by weighted edges. Each neuron also has an associated 'activation' value, though this is only meaningful while the model is being inferenced. Figure \ref{fig:1} provides a pictorial representation of this architecture. There are three types of layers in the networks:
\begin{itemize}
	\item \textbf{Input Layer} ($L_1$): The first layer of the network is the input layer. Here, each neuron's activation value corresponds directly to the value of a distinct feature in the datapoint currently being inferenced. From this, we derive the relation:
	\begin{flalign}
		|L_1| = in\_dim
	\end{flalign}
	Where $in\_dim$ is the dimensionality (number of features) of a datapoint in our dataset.

	\item \textbf{Hidden Layers} ($L_2, \dots, L_{n_{hidden} + 1}$): The layers between the first and last (exclusive) are the hidden layers. \textbf{The model's architecture is parametrised by the number of hidden layers ($n_{hidden}$), and the size of each hidden layer (the number of neurons in it)}, which can both be any integer (though these are usually selected very carefully depending on the problem). On each of these layers, a non-linear activation function is applied after activations are calculated, allowing the network to capture non-linear relations (eg. ReLU).
	
	\item \textbf{Output Layer} ($L_{n_{hidden + 2}}$): The final layer of the network is the output layer. Here, each neuron's activation values correspond (through some function) to the likelihood that the datapoint currently being inferenced belongs to a distinct class. This unknown function is called a non-linear activation function, and \textbf{is another parameter in the model's architecture}, with the softmax function being most commonly applied to classification tasks. We also have the relation:
	\begin{flalign}
		|L_{n_{hidden + 2}}| = out\_dim
	\end{flalign}
	Where $out\_dim$ is the number of distinct classes our datapoint can belong to.
\end{itemize}
While the model architecture is parametrised by the number of hidden layers, size of each hidden layer, and the choice of activation function to be applied at the output layer, being a machine learning architecture, it also contains a large number of trainable parameters. We omit discussion of the training procedure, due to it being out of scope for this project.\bigskip

The output of the training are the trainable parameters, the weights \& biases. As mentioned earlier, $\forall i > 1$, each neuron in $L_i$ is connected to each neuron in the layer $L_{i-1}$ by a weighted edge. Then, the weights matrix for this layer is simply a submatrix of the adjacency matrix representation of the subgraph induced by these adjoining edges (if we discard the matrix elements corresponding to nodes not joined by an edge, i.e., nodes in the same layer). Then, if $|L_i| = n_i, |L_{i - 1}| = n_{i - 1}$, the weights matrix for this layer $W_i: n_i \times n_{i - 1}$. The propagation formula for the FFN at this layer is given by:
\begin{flalign}
	X_i = \text{Activation}(W_iX_{i - 1} + B_i\label{eq:3})
\end{flalign}
Where $X_i, X_{i - 1}$ are the neuron activations at layers $i, (i - 1)$ respectively (these are column vectors, so $X_i: n_i \times 1, X_{i - 1}: n_{i - 1} \times 1$), $B_i: n_i \times 1$ is the bias column vector for this layer (trained parameter), and Activation($\cdot$) is the non-linear activation function (preserves matrix dimension). The knowledge that activation vectors ($X_i$s) are represented as column vectors, along with an understanding of matrix multiplication can be used to derive the following lemmas:
\begin{lemma} \label{lem:1}
	Given the weights matrix $W_i: a \times b$ at layer $L_i$, we have that $n_{i - 1} = b$.
\end{lemma} \label{lem:2}
\begin{lemma}
	Given the weights matrix $W_i: a \times b$ at layer $L_i$, we have that $n_i = a$.
\end{lemma}
Similarly, using our knowledge of matrix-matrix addition, we have:
\begin{lemma} \label{lem:3}
	Given the weights matrix $W_i: a \times b$ at layer $L_i$, the biases vector $B_i$ has the shape $B_i: a \times 1$.
\end{lemma}

\section{Literature Survey}
\subsection{Existing Neural Network Libraries}
Before we implement custom logic for reconstructing and running inference on a FFN, we first check the Haskell ecosystem for any existing neural network/machine learning libraries.\bigskip

We immediately found \href{https://hackage.haskell.org/package/neural}{neural}, a pure haskell framework for training and inferencing on various kinds of neural networks. Additionally, we find \href{https://github.com/HuwCampbell/grenade}{grenade}, \href{https://hackage.haskell.org/package/neural-network-hmatrix}{neural-network-hmatrix}, and \href{https://hackage.haskell.org/package/hnn}{hnn}. Unfortunately, all of them have the same problems.\bigskip

For our chosen problem, they are all overkill, attempting to provide a generic framework for training and using neural networks. The training part adds the majority of the overhead to the interfaces provided by these libraries, but we do not require any training support for this project. Adding to this, our project only cares about a very specific architecture, the FFN, whereas these projects pursue support for many architectures, leading to a less overall flexible (programmatic) interface for our case, compared to one we design on our own.\bigskip

For these reasons, we choose to implement the inference without the support of a deep learning library.

\subsection{Fast Linear Algebra in Haskell} \label{sec:3.2}
As seen in \eqref{eq:3}, the computation we perform during inference is essentially a series of matrix multiplications (and additions). In order to optimise this, our first task is to determine how haskell supports these operations.\bigskip

\begin{table}
	\centering \begin{tabular}{|c|c|c|c|}
		\hline \textbf{Library} & $n = 10$ & $n = 50$ & $n = 100$\\
		\hline DLA & 2.65us & 289.0us & 2.24ms\\
		\hline Hmatrix & 1.32us & 55.8us & 292.0us\\
		\hline NumHask & 714.0us & 63.5ms & 593.0ms\\
		\hline Massiv & 12.0us & 205.0us & 1.52ms\\
		\hline Massiv (Parallel) & 76.1us & 220.0us & 866.0us\\
		\hline Matrix & 12.6us & 1.1ms & 8.44ms\\
		\hline Naive C Implementation & 51us & 323us & 4.78ms\\
		\hline
	\end{tabular}
	\caption{\label{tab:1} \href{https://github.com/Magalame/fastest-matrices}{Linear Algebra Library Benchmark (matrix multiplication of size $n \times n$)}}
\end{table}
At a cursory glance, it can be seen that \href{https://wiki.haskell.org/Applications_and_libraries/Mathematics#Linear_algebra}{there are various linear algebra libraries available for use with haskell}. In order to hone down on one library for usage, \href{https://github.com/Magalame/fastest-matrices}{this benchmark from 2015} is the best evidence we have to go off (short of running our own benchmark). Table \ref{tab:1} displays their results for the matrix multiplication task.\bigskip

According to this benchmark, the \href{https://hackage.haskell.org/package/hmatrix}{Hmatrix} library achieves SOTA performance for both the matrix multiplication and repeated matrix multiplication task (in a single-threaded setup). Then, we choose to utilise this library for maximising performance.

\section{Problem Statement}
\subsection{Problem Statement}
Develop a classification model over the Iris Dataset and store the model. Then write a Haskell code to restore the model, input new data (based on the four features of the iris) and generate a prediction in real time.
\begin{enumerate}
	\item Input: The Iris Dataset.
	\item Output: Real-time prediction for the class of the flower of a new data row containing the sepal width, petal width, sepal length and petal length.
	\item Method: You can use Python to train and save a classification model (SVM or NN). However, restoring the model and the real-time prediction of a new data row has to be written only in Haskell.
\end{enumerate}

\subsection{Objectives}
Based on this problem statement \& initial motivation, we aim to develop a system which:
\begin{itemize}
	\item Should be able to load a trained classification model.
	\item Given any new data point of the same format as the iris dataset by the user, should be able to use the loaded model to make a prediction on this data and report it to the user.
	\item Should be as performant as possible, since the problem description suggests that the application domain is a real-time system.
	\item Should be simple to use for a layperson.
	\item Should generalise beyond the iris dataset.
\end{itemize}

\section{Scope}
In order to fulfill our objectives, we have developed a system with the following featureset:
\begin{enumerate}
	\item Parsing capabilities to restore the shape, weights, and biases of \textbf{an arbitrary} FFN classifier from a file.
	\item Interactive \& batched modes of performing inference. \begin{itemize}
		\item \textbf{Interactive}: The user enters a single new datapoint and gets predictions for it in real-time. Structured as a Read-Eval-Print-Loop (REPL).
		\item \textbf{Batched}: The user provides many datapoints in a csv file, and receives predictions for each point.
	\end{itemize}
	\item An evaluation mode: an extension of batched inference mode which also takes in expected outputs, and determines model accuracy.
\end{enumerate}
To expand on point 1, since the type of model (FFN) is known, we can generalise the system to work with any classifier following that architecture, since this is independent of the particular dataset used, as long as that dataset only contains numeric feature. This allows the system to be reused for similar tasks beyond the Iris dataset, providing us the generality we desire.\bigskip

However, we do limit the class of models we allow to using ReLU (at the hidden layers) \& Softmax (at the output layer) activations, which covers most, but not all, FFN classifiers.

\begin{figure}
	\includegraphics[width=0.85\pdfpagewidth]{../images/modules.png}
	\caption{\label{fig:2}the high-level design of the system}
\end{figure}

\section{Methodology \& Design}
\subsection{Architecture}
Since we are only concerned with running the inference for a new row for this dataset, we train our classification models separately (for testing purposes) before runtime. At runtime, we restore the structure, weights, and biases of the model, and push the given feature vector through the model in order to arrive at the inferred output.\bigskip

Then, the system can broadly be divided into two modules, the training module \& the inference module. These operate independently from each other, with the only interface between them being the inference module taking the training module's output (model structure, weights, biases) as its input, which is then used to reconstruct the model. Figure \ref{fig:2} illustrates the general flow of this architecture.

\subsection{Trained Model Format}
From Section \ref{sec:2.2}, we know that a feedforward neural network is comprised of several layers of neurons, with each layer (besides the input layer) having a matrix of weights \& a vector of biases associated with itself.\bigskip

Then, our naive model format is a binary file structured as follows:
\begin{verbatim}
number of layers
for each non-input layer in the network:
    weights matrix rows
    weights matrix columns
    weights matrix
    biases vector length
    biases vector
\end{verbatim}
Immediately, from lemma \ref{lem:3}, we can reduce this to a compacted format:
\begin{verbatim}
number of layers
for each non-input layer in the network:
    weights matrix rows
    weights matrix columns
    weights matrix
    biases vector
\end{verbatim}
We can make another space optimisation by noticing that lemma \ref{lem:1} allows us to infer the number of columns in the weights matrix of one layer from the number of neurons in the previous layer. Since the format is defined iteratively over layers, we can exploit this relation while parsing using memoisation. All we need to begin is the number of neurons in the input layer.
\begin{verbatim}
number of layers
number of neurons in input layer
for each non-input layer in the network:
    weights matrix rows
    weights matrix
    biases vector
\end{verbatim}
Since each stored integer here is 4 bytes wide, we have brought down the size of our format from $4\times (3l + 1) + D = 12l + D + 4$ bytes, where $l$ is the number of non-input layers and $D$ is the constant size of the actual data (weights \& biases) to $4\times(l + 2) + D = 4l + D + 8$ bytes, which scales much better with number of layers. Then, we have managed to enhance the information density of our format using our observations about the network architecture.

\subsection{Training Module}
The purpose of the training module is to provide us with a set of models we can use to test our inference module. As such, we don't strictly require a high performance model. However, we still perform some basic hyperparameter selection after loading the dataset in using a 5-fold cross validation approach to determine the shape of the network (number \& size of hidden layers).\bigskip

The model architecture used is the one mentioned above, a simple feedforward network, a sequence of linear layers using the ReLU activation function between the hidden layers, and the softmax activation at the end. Using a grid search, we find the top 5 performing shapes, and also one poorly performing shape, out of a generated set of possible shapes for the network.\bigskip

Before fitting 6 models using these shapes on the data, the data is transformed by converting the output classes column from textual labels to numerical labels, to avoid any ordering ambiguities arising during the training process. This transformed data is then split into training \& test sets, and we fit the models on the training set.\bigskip

Finally, we write the networks' weights and biases into model files, which can then be used by the inference module.

\subsection{Inference Module}
The inference module constitutes the bulk of the codebase. It produces an executable with the following signature:
\begin{verbatim}
inference-exe <path_to_model_file> [<path_to_batch_csv>] [<path_to_labels_csv>]
\end{verbatim}
If the second, optional argument is omitted, it starts in interactive mode. Otherwise, it starts in batched mode. If the third argument is provided, it runs in evaluation mode.\bigskip

In either case, its first task is to restore the model from the given model file. Here, we utilise the parsing support module, which is written using parser combinators. Further implementation details of the parser are discussed in the Test Plan \& Prototype Details section. \bigskip

Through this parsing process, the model shape, and its parameters are loaded into a data structure which takes the form of a list of layers, where each layer stores its input \& output shapes, and the weights \& biases matrices.\bigskip

Next, based on which mode it was started in, the system either enters a REPL or parses the input batch csv file it was provided (again, with the help of the parsing support module). In either mode, once the feature vector(s) are acquired, the tensor support module is used to perform the sequence of computations that lead to the final prediction.\bigskip

The final output (for each feature vector) is the predicted class and the probabilities associated with each class. If the program was started in evaluation mode, it will also report an accuracy metric, that is, how many of the predicted outputs matched their correct labels as given in the labels csv file, as a percentage.

\section{Work Done}
\subsection{Tooling}
\subsubsection{Training Module}
The training module is written in python, using numpy, pandas, and sklearn to perform the data processing \& model training/selection.\bigskip

\subsubsection{Inference Module}
The inference module is written in Haskell. We use the Haskell Tool Stack (or just Stack) as our build tool. The following dependencies are used:
\begin{itemize}
	\item \textbf{hmatrix}\\
	As described earlier in \ref{sec:3.2}, this library provides vector \& matrix types, along with the definitions for common mathematical operations on them, including matrix multiplication, conversions, etc.\bigskip

	It is the backbone of tensor support module, which is used to finally perform the inference once the inputs have been parsed.

	\item \textbf{\href{https://hackage.haskell.org/package/attoparsec}{attoparsec}}\\
	Attoparsec is a highly performant parser combinator library, focusing on complex text \& binary file formats. Attoparsec forms the foundations of the parsing support module, which is used for parsing both the model files \& csv files for batched input/evaluation mode.
\end{itemize}

\subsubsection{Testing/Miscellaneous}
A mix of bash \& python scripts is used to implement the end-to-end tests and certain convenient utilities (eg. infer.py for loading and making predictions using sklearn, used to verify inference module). We also utilise the \href{https://hackage.haskell.org/package/timeit}{timeit} package to benchmark our performance.

\subsection{Test Suite}
Due to the large number of disjoint moving parts in the project, the plan is to implement a comprehensive test suite consisting of unit \& end-to-end tests. Additionally, we will be profiling our system to ensure it meets the real-time requirements.

\subsubsection{Unit Testing}
First, we note that the training module does not require testing, due to its simplicity and it almost exclusively using sklearn library functions. For the inference module, all the parser functions, and many of the inference functions are complex enough to warrant unit testing. We plan to implement these using HSpec, which is the testing framework Stack provides.\bigskip

In order to cover the large space of inputs for these functions, we plan to heavily depend on generated test suites, as opposed to hand-crafted ones.

Consider two examples:
\begin{itemize}
	\item \textbf{For the matrix parser:} We generate random real-valued matrices, stringify them, run them through the parser, and verify that the same matrix is recovered.
	\item \textbf{For batch softmax:} Use single-vector inference softmax as oracle and verify batch softmax.
\end{itemize}

\subsubsection{End-to-End Testing}
We use an end-to-end test to verify the correctness of our inference module. Here, we treat our sklearn model as an oracle, randomly generating many batches of feature vectors (in addition to the existing test split), running them through both our oracle \& the inference module, and comparing the output activations using mean square error.

\subsubsection{Profiling}
To profile our inference, we run batch prediction on networks with various shapes, timing how long the inference itself takes (not the parsing), and compare it with the time taken to run the same inferences in python with scikit-learn.

\subsection{Implementation}
\subsubsection{Project Structure}
\begin{verbatim}
src
|
-- inference              <--- inference module
|  |
|  -- app/Main.hs         <--- driver code
|  -- src/Lib.hs          <--- tensor support module
|  -- src/Parser.hs       <--- parsing support module
|  -- test/Spec.hs        <--- test suite
|  ...
-- models                 <--- pickled sklearn models (for testing)
-- weights                <--- model files
-- infer.py               <--- sklearn inference helper
-- bench.ipynb            <--- time comparison benchmark
-- e2e.ipynb              <--- end-to-end test implementation
-- gen.ipynb              <--- dataset generator for testing
-- iris*.csv              <--- datasets
-- train_model.ipynb      <--- training module
\end{verbatim}

\subsubsection{Functionality/User Flow}
\subsubsection{Parsing a model file}

\subsubsection{Challenges \& Mitigation}
\subsubsection{Migrating Libraries}
\subsubsection{LAPack \& Blas}
\subsubsection{Batch inference}


\section{Results \& Discussion}
\subsection{Example Runs}
{
	\begin{center}
		\includegraphics[width=\textwidth]{../images/examples.png}
	\end{center}
	\begin{center}
		Interactive Mode
	\end{center}
	\begin{center}
		\includegraphics[width=\textwidth]{../images/examplebatch.png}
	\end{center}
	\begin{center}
		Batched Mode
	\end{center}
}

\subsection{Testing Outcomes}

\subsection{Profiling Outcomes}

\subsection{Limitations}
\begin{itemize}
	\item Lack of runtime checks for user input, leading to a restrictive input format \& several non-graceful exits, without any useful feedback to the user.
	\item The csv input in batch mode can't end on a blank line (parser bug)
	\item Batch mode doesn't provide summary statistics
	
	\item Functionality is limited to feed-forward classifiers, though this is purely due to presentation of the output
	\item The command-line interface, while designed for usability, still may serve as a barrier for the most technically-challenged individuals
	\item Requires the weights to be in a well-known format, and for the user to locate the weights file manually
	\item Class labels aren't currently considered
\end{itemize}

\section{Conclusions}

\section{Extensions \& Future Work}
\begin{itemize}
	\item Infer the format of the weights file based on a few well-known formats (.pkl, .pth, .safetensors), instead of only working for our custom format. This requires more than just looking at the extension, as we have to further infer where the weights and biases of each layer lie based on the keys in the file. 
\end{itemize}

\section{References}

\end{document}

