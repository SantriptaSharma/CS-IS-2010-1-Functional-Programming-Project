\documentclass[12pt, titlepage]{article}
\usepackage[a4paper, total={6.5in, 10in}]{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}

\usepackage{tocloft}

\cftsetindents{section}{0em}{2em}
\cftsetindents{subsection}{0em}{2em}

\renewcommand\cfttoctitlefont{\hfill\Large\bfseries}
\renewcommand\cftaftertoctitle{\hfill\mbox{}\vspace*{6mm}}

\setcounter{tocdepth}{2}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Haskell,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{braket}
\usepackage{qcircuit}
\usepackage{graphicx}

\newcommand{\code}{CS-IS-2010-1}
\newcommand{\name}{Functional Programming Report}
\newcommand{\me}{Santripta Sharma}

\newcommand{\prob}[1]{\mathbb{P}\left[#1\right]}

\newcommand{\kz}{\ket{0}}
\newcommand{\ko}{\ket{1}}
\newcommand{\kp}{\ket{\phi}}
\newcommand{\ks}{\ket{\psi}}
\newcommand{\kso}{\ket{\psi_1}}
\newcommand{\kst}{\ket{\psi_2}}

\newcommand{\rec}[1]{\frac{1}{#1}}

\newtheorem{lemma}{Lemma}

\newenvironment*{qparts}{\begin{enumerate}[label=(\alph*)]}{\end{enumerate}}

\title{Inference \& Evaluation Engine for Feed-Forward Neural Networks in Haskell\vspace*{2mm}

{\large \code: \name}

{\large Supervisor: \href{https://www.ashoka.edu.in/profile/partha-pratim-das/}{Partha Pratim Das}}}
\author{\textbf{\me}\\(1020211136)}
\date{\today}

\markright{\code\hspace{0.5in}\name\hspace{0.5in}\me}

\parindent=0pt

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Introduction}
Through the course of this project, we have developed a command-line tool to run inference on (or evaluate) an arbitrary feed-forward neural network (for classification tasks). The primary goal for the project was to gain familiarity with haskell (and the functional programming paradigm) by implementing a medium-sized project using it.\bigskip

In this regard, the project has been quite succesful, forcing me to cover a lot of the bases in a relatively short time. The entire process took me from the basics, performing command-line I/O, and using the haskell tool stack, all the way to writing parser combinators for binary data, using monads, and implementing test suites with HUnit.\bigskip

Besides the learning outcomes from the project, the tool that has been developed is also useful in practical settings. It has been designed \& developed keeping in mind optimality, generalizability, and usability. An example use case of this tool is one where the user has been handed a trained classifier network (by, say, a tech-consultancy firm), and they wish to verify its performance on their data, without writing an evaluation/inference script on their own (due to time constraints or inexperience with the technology).

\section{Background \& Motivation}
\subsection{Motivation}
Today, Machine Learning has left no domain untouched. What initially started as a niche subfield of statistics has now found its application in nearly every sector \& discipline. This poses a simple problem: A large chunk of the users of these models are not familiar with how they work or how to use them. This has led to widespread black-boxing, where the development/training of the model is often outsourced to someone with this technical knowledge, with the 'client' being given instructions on how to use the model.\bigskip

Besides the inherent security risk, where the external party could place a backdoor into the model, this approach also forces the client to take the external party at their word when it comes to the effectiveness of the model, at least initially, which opens the door up to being defrauded. Additionally, in some cases, the instructions provided may be obtuse, rendering the client unable to use the model.\bigskip

Therefore, there is a need to be served for providing a solution that can perform inference and evaluation on these models, while also being friendly to users who aren't very tech-savvy. In this project, we particularly focus on a popular subclass of ML models, the feed-forward classifier, which is a generalisation of linear/logistic regression, and a foundational piece in most deep neural networks, with this unique position leading it to be one of the most common architectures for these models.

\subsection{Background: Feed-Forward Networks}
\begin{figure}
	\includegraphics[width=0.8\pdfpagewidth]{../images/FFN-structure.png}
	\caption{\label{fig:1}a generic FFN with $n_{hidden} = 2$}
\end{figure}
The Feed-Forward Network (FFN) is a versatile network architecture for deep neural networks, able to be used as both classifiers or regressors (though this project restricts its scope to the former). It is comprised of a set of layers ($L = \{L_1, L_2, \dots, L_{n_{hidden} + 2}\}$), each containing nodes, called 'neurons', each of which are connected to every neuron in the next layer by weighted edges. Each neuron also has an associated 'activation' value, though this is only meaningful while the model is being inferenced. Figure \ref{fig:1} provides a pictorial representation of this architecture. There are three types of layers in the networks:
\begin{itemize}
	\item \textbf{Input Layer} ($L_1$): The first layer of the network is the input layer. Here, each neuron's activation value corresponds directly to the value of a distinct feature in the datapoint currently being inferenced. From this, we derive the relation:
	\begin{flalign}
		|L_1| = in\_dim
	\end{flalign}
	Where $in\_dim$ is the dimensionality (number of features) of a datapoint in our dataset.

	\item \textbf{Hidden Layers} ($L_2, \dots, L_{n_{hidden} + 1}$): The layers between the first and last (exclusive) are the hidden layers. \textbf{The model's architecture is parametrised by the number of hidden layers ($n_{hidden}$), and the size of each hidden layer (the number of neurons in it)}, which can both be any integer (though these are usually selected very carefully depending on the problem)
	
	\item \textbf{Output Layer} ($L_{n_{hidden + 2}}$): The final layer of the network is the output layer. Here, each neuron's activation values correspond (through some function) to the likelihood that the datapoint currently being inferenced belongs to a distinct class. This unknown function is called a non-linear activation function, and \textbf{is another parameter in the model's architecture}, with the softmax function being most commonly applied to classification tasks. We also have the relation:
	\begin{flalign}
		|L_{n_{hidden + 2}}| = out\_dim
	\end{flalign}
	Where $out\_dim$ is the number of distinct classes our datapoint can belong to.
\end{itemize}
While the model architecture is parametrised by the number of hidden layers, size of each hidden layer, and the choice of activation function to be applied at the output layer, being a machine learning architecture, it also contains a large number of trainable parameters. We omit discussion of the training procedure, due to it being out of scope for this project.\bigskip

The output of the training are the trainable parameters, the weights \& biases. As mentioned earlier, $\forall i > 1$, each neuron in $L_i$ is connected to each neuron in the layer $L_{i-1}$ by a weighted edge. Then, the weights matrix for this layer is simply an adjacency matrix representation of the subgraph induced by these adjoining edges. Then, if $|L_i| = n_i, |L_{i - 1} = n_{i - 1}$, the weights matrix for this layer $W_i: n_i \times n_{i - 1}$. The propagation formula for the FFN at this layer is given by:
\begin{flalign}
	X_i = W_iX_{i - 1} + B_i\label{eq:3}
\end{flalign}
Where $X_i, X_{i - 1}$ are the neuron activations at layers $i, (i - 1)$ respectively (these are column vectors, so $X_i: n_i \times 1, X_{i - 1}: n_{i - 1} \times 1$), and $B_i: n_i \times 1$ is the bias column vector for this layer (trained parameter). The knowledge that activation vectors ($X_i$s) are represented as column vectors, along with an understanding of matrix multiplication can be used to derive the following lemmas:
\begin{lemma}
	Given the weights matrix $W_i: a \times b$ at layer $L_i$, we have that $n_{i - 1} = a$.
\end{lemma}
\begin{lemma}
	Given the weights matrix $W_i: a \times b$ at layer $L_i$, we have that $n_i = b$.
\end{lemma}
Which we will use later.

\section{Literature Survey}
\subsection{Existing Neural Network Libraries}
Before we implement custom logic for reconstructing and running inference on a FFN, we first check the Haskell ecosystem for any existing neural network/machine learning libraries.\bigskip

We immediately found \href{https://hackage.haskell.org/package/neural}{neural}, a pure haskell framework for training and inferencing on various kinds of neural networks. Additionally, we find \href{https://github.com/HuwCampbell/grenade}{grenade}, \href{https://hackage.haskell.org/package/neural-network-hmatrix}{neural-network-hmatrix}, and \href{https://hackage.haskell.org/package/hnn}{hnn}. Unfortunately, all of them have the same problem.\bigskip

For our chosen problem, they are all overkill, attempting to provide a generic framework for training and using neural networks. The training part adds the majority of the overhead to the interfaces provided by these libraries, but we do not require any training support for this project. Adding to this, our project only cares about a very specific architecture, the FFN, whereas these projects pursue generalisability, leading to a less overall flexible (programmatic) interface for our case, compared to one we design on our own.\bigskip

For these reasons, we choose to implement the inference without the help of a deep learning library.

\subsection{Fast Linear Algebra in Haskell}
As seen in \eqref{eq:3}, the computation we perform during inference is essentially a series of matrix multiplications (and additions). In order to optimise this, our first task is to determine how haskell supports these operations.\bigskip

\begin{table}
	\centering \begin{tabular}{|c|c|c|c|}
		\hline \textbf{Library} & $n = 10$ & $n = 50$ & $n = 100$\\
		\hline DLA & 2.65us & 289.0us & 2.24ms\\
		\hline Hmatrix & 1.32us & 55.8us & 292.0us\\
		\hline NumHask & 714.0us & 63.5ms & 593.0ms\\
		\hline Massiv & 12.0us & 205.0us & 1.52ms\\
		\hline Massiv (Parallel) & 76.1us & 220.0us & 866.0us\\
		\hline Matrix & 12.6us & 1.1ms & 8.44ms\\
		\hline Naive C Implementation & 51us & 323us & 4.78ms\\
		\hline
	\end{tabular}
	\caption{\label{tab:1} \href{https://github.com/Magalame/fastest-matrices}{Linear Algebra Library Benchmark (matrix multiplication of size $n \times n$)}}
\end{table}
At a cursory glance, it can be seen that \href{https://wiki.haskell.org/Applications_and_libraries/Mathematics#Linear_algebra}{there are various linear algebra libraries available for use with haskell}. In order to hone down on one library for usage, \href{https://github.com/Magalame/fastest-matrices}{this benchmark from 2015} is the best evidence we have to go off (short of running our own benchmark). Table \ref{tab:1} displays their results for the matrix multiplication task.\bigskip

According to this benchmark, the \href{https://hackage.haskell.org/package/hmatrix}{Hmatrix} library achieves SOTA performance for both the matrix multiplication and repeated matrix multiplication task. Then, we choose to utilise this library for maximising performance.

\section{Problem Statement}
\subsection{Problem Statement}
Develop a classification model over the Iris Dataset and store the model. Then write a Haskell code to restore the model, input new data (based on the four features of the iris) and generate a prediction in real time.
\begin{enumerate}
	\item Input: The Iris Dataset.
	\item Output: Real-time prediction for the class of the flower of a new data row containing the sepal width, petal width, sepal length and petal length.
	\item Method: You can use Python to train and save a classification model (SVM or NN). However, restoring the model and the real-time prediction of a new data row has to be written only in Haskell.
\end{enumerate}

\subsection{Objectives}
Based on this problem statement \& initial motivation, we aim to develop a system which:
\begin{itemize}
	\item Should be able to load a trained classification model.
	\item Given any new data point of the same format as the iris dataset by the user, should be able to use the loaded model to make a prediction on this data and report it to the user.
	\item Should be as performant as possible, since the problem description suggests that the application domain is a real-time system.
	\item Should be simple to use for a layperson.
	\item Should generalise beyond the iris dataset.
\end{itemize}

\section{Scope}
Generalising from the requirements, we offer the following featureset:
\begin{enumerate}
	\item Parsing capabilities to restore the shape, weights, and biases of \textbf{an arbitrary} FFN-based classifier from a file.
	\item Interactive \& batched modes of performing inference. \begin{itemize}
		\item \textbf{Interactive}: The user enters a single new datapoint and gets predictions for it in real-time. Structured as a Read-Eval-Print-Loop (REPL).
		\item \textbf{Batched}: The user provides many datapoints in a csv file, and receives predictions for each point.
	\end{itemize}
	\item An evaluation mode: an extension of batched inference mode which also takes in expected outputs, and determines model accuracy.
\end{enumerate}
To briefly expand on point 1, since the type of model (FFN) is known, we can generalise the system to work with any classifier following that architecture, since this is independent of the particular dataset used, as long as that dataset only contains numeric feature. This allows the system to be reused for similar tasks beyond the Iris dataset, providing us the generality we desire.\bigskip

\begin{figure}
	\includegraphics[width=0.85\pdfpagewidth]{../images/modules.png}
	\caption{\label{fig:2}the high-level design of the system}
\end{figure}

\section{Methodology \& Design}
\subsection{Architecture}
Since we are only concerned with running the inference for a new row for this dataset, we train our classification models separately (for testing purposes) before runtime. At runtime, we restore the structure, weights, and biases of the model, and push the given feature vector through the model in order to arrive at the inferred output.\bigskip

Then, the system can broadly be divided into two modules, the training module \& the inference module. These operate independently from each other, with the only interface between them being the inference module taking the training module's output (model structure, weights, biases) as its input, which is then used to reconstruct the model. Figure \ref{fig:2} illustrates the general flow of this architecture.

\subsection{Trained Model Format}
For the prototyping stage, we opt to use a plaintext format for the trained model parameters, since this allows easier debugging and editing. This will eventually be transitioned to a binary format.\bigskip

We know that a feedforward neural network is comprised of several layers of neurons, with each layer (besides the input layer) having a matrix of weights \& a vector of biases associated with itself. Further, using the propagation formula $Wx + b$, we can infer the incoming and outgoing dimension of each stage from these matrices alone.\bigskip

Then, our format is simply a text file structured as follows:
\begin{verbatim}
for each layer in the network:
    weights matrix
    biases vector
\end{verbatim}
For example, this is the model file for a network with a single hidden layer of size 5:
\begin{verbatim}
[[ 0.23400233  0.46448114 -0.18169046 -1.02006724]
[ 0.91292487 -0.83703552  0.46820428  1.07709024]
[ 0.77124784  0.60032112 -1.04011005 -1.06176582]
[ 0.04108179  0.0236362  -0.6077499  -0.57499983]
[ 0.41324034  0.71752045 -1.10962444  0.24183919]]
[ 1.05104845 -1.15333131  1.42181299  0.46979225 -0.31311588]
[[ 0.54906348 -1.2267044   1.117347   -0.7446324   1.17420772]
[-0.24563245 -0.00604219  1.02716034  0.51893559 -0.44818289]
[-0.79535198  0.72517445 -1.57061672  0.55235219  0.11317021]]
[ 0.18887961  0.43315065 -0.37568123]   
\end{verbatim}
From the first layer's weights having four columns, we can infer that there are four input features, and from the last layer's weights having three rows, we can infer that there are three output classes. This ability to infer input \& output shape at parse-time is what allows this structure to be independent of the dataset the model is trained for.\bigskip

It is also worth stating that the system currently makes the assumption that the network uses a ReLU activation at every layer except the last, where softmax is used (as is customary for multiclass classification). Therefore, in truth, the space of models the system accepts is a little more restricted than claimed in the specification (since in a classification model, the ReLU here could very well be replaced by a variety of alternative activation functions).\bigskip

We can now dissect the modules separately.

\subsection{Training Module}
The purpose of the training module is to provide us with a set of models we can use to test our inference module. As such, we don't strictly require a high performance model. However, we still perform some basic hyperparameter selection after loading the dataset in using a 5-fold cross validation approach to determine the shape of the network (number \& size of hidden layers).\bigskip

The model architecture used is the one mentioned above, a simple feedforward network, a sequence of linear layers using the ReLU activation function between the hidden layers, and the softmax activation at the end. Using a grid search, we find the top 5 performing shapes, and also one poorly performing shape, out of a generated set of possible shapes for the network.\bigskip

Before fitting 6 models using these shapes on the data, the data is transformed by converting the output classes column from textual labels to numerical labels, to avoid any ordering ambiguities arising during the training process. This transformed data is then split into. into training \& test sets, and fit the models on the training set.\bigskip

Finally, we write the networks' weights and biases into model files, which can then be used by the inference module.

\subsection{Inference Module}
The inference module constitutes the bulk of the codebase. It produces an executable with the following signature:
\begin{verbatim}
inference-exe <path_to_model_file> [<path_to_batch_csv>]
\end{verbatim}
If the second, optional argument is omitted, it starts in interactive mode. Otherwise, it starts in batched mode.\bigskip

In either case, its first task is to restore the model from the given model file. Here, we utilise the parsing support module, which is written using parser combinators. Since the current model file format is plaintext, it is well suited to parsing by simple regular expressions. However, we opt to use parser combinators to facilitate a quicker/smoother transition to the binary model file format planned for the final project, which now only requires a slight rewrite of the parser. Additionally, for more complex formats (like our model file), the parser combinator approach admits more scope for unit testing. 

Further implementation details of the parser are discussed in the Test Plan \& Prototype Details section. \bigskip

Through this parsing process, the model shape, and its parameters are loaded into a data structure which takes the form of a list of layers, where each layer stores its input \& output shapes, and the weights \& biases matrices.\bigskip

Next, based on which mode it was started in, the system either enters a REPL or parses the input batch csv file it was provided (again, with the help of the parsing support module). In either mode, once the feature vector(s) are acquired, the tensor support module is used to perform the sequence of computations that lead to the final prediction.\bigskip

The final output (for each feature vector) is the predicted class and the probabilities associated with each class. In the case of batch mode, we also report summary statistics, how many vectors belonged to each class.

\section{Work Done}
\subsection{Tooling}
\subsubsection{Training Module}
The training module is written in python, using numpy, pandas, and sklearn to perform the data processing \& model training/selection.\bigskip

\subsubsection{Inference Module}
The inference module is written in Haskell. We use the Haskell Tool Stack (or just Stack) as our build tool. The following dependencies are used:
\begin{itemize}
	\item \textbf{matrix \& vector}\\
	As the name suggests, these two packages provide a Matrix \& Vector type respectively, also providing the definitions for common mathematical operations on them, including matrix multiplication, converting between the two types, etc.\bigskip

	Together, they form the tensor support module, which is used to finally perform the inference once the inputs have been parsed.

	\item \textbf{parsec}\\
	Parsec is Haskell's standard parser combinator library, allowing us to write atomic parsers and compose them together. Parsec forms the foundations of the parsing support module, which is used for parsing both the model files \& csv files for batched input mode.\bigskip

	Parsec has a variant called attoparsec, which is designed for binary file parsing. Later iterations of the project will swap parsec for attoparsec (for the task of model file parsing), as the model file format switches from a plaintext to a binary file format.

	Discussion of the specifics of the parser implementations is deferred to the Prototype Details section.
\end{itemize}

\subsubsection{Testing/Miscellaneous}
A mix of bash \& python scripts is used to implement the end-to-end tests and certain convenient utilities (eg. infer.py for loading and making predictions using sklearn, used to verify infer module). 

\subsection{Test Suite}
Due to the large number of disjoint moving parts in the project, the plan is to implement a test suite consisting of unit \& end-to-end tests. Additionally, we will be profiling our system to ensure it meets the real-time requirements.

\subsubsection{Unit Testing}
First, we note that the training module does not require testing, due to its simplicity and it almost exclusively using sklearn library functions. For the inference module, all the parser functions, and many of the inference functions are complex enough to warrant unit testing. We plan to implement these using HSpec, which is the testing framework Stack provides.\bigskip

In order to cover the large space of inputs for these functions, we plan to heavily depend on generated test suites, as opposed to hand-crafted ones.

Consider two examples:
\begin{itemize}
	\item \textbf{For the matrix parser:} We generate random real-valued matrices, stringify them, run them through the parser, and verify that the same matrix is recovered.
	\item \textbf{For batch softmax:} Use single-vector inference softmax as oracle and verify batch softmax.
\end{itemize}

\subsubsection{End-to-End Testing}
We use an end-to-end test to verify the correctness of our inference module. Here, we treat our sklearn model as an oracle, randomly generating many batches of feature vectors (in addition to the existing test split), running them through both our oracle \& the inference module, and comparing the output activations.

\subsubsection{Profiling}

\subsection{Implementation}
\subsubsection{Project Structure}
\begin{verbatim}
src
|
-- inference <--- inference module
|  |
|  -- app/Main.hs    <--- driver code
|  -- src/Lib.hs     <--- tensor support module
|  -- src/Parser.hs  <--- parsing support module
|  ...
-- models    <--- pickled sklearn models (for testing)
-- weights   <--- model files
-- infer.py  <--- sklearn inference helper
-- iris*.csv <--- datasets
-- train_model.ipynb <--- training module
\end{verbatim}

\subsubsection{Feature Set}
The prototype implements the following features:
\begin{itemize}
	\item Complete restoration of any feed-forward classifier, given its parameters in the model file format, assuming it uses ReLU + Softmax activations.
	\item Interactive mode REPL, where the user can enter new datapoints, and get prediction outputs (activations + class label) for them.
	\item Batch mode, where the user can provide a csv file of datapoints to run inference on, and get prediction outputs for each point.
\end{itemize}

\subsubsection{Functionality/User Flow}
\subsubsection{Parsing a model file}
The power of a parser combinator comes from its characteristic composability of various atomic parsers, which can each be individually tested, building up to the final parser. To illustrate this, here's the parser for a number within a model file.
\begin{lstlisting}
number :: Parser Double
number = do
	sign <- option "" $ string "-"
	int <- many1 digit <?> "integer part"
	dec <- option "" $ do
		void $ char '.'
		frac <- many1 digit
		return $ '.' : frac
	pow <- option "" $ do
		void $ char 'e'
		esign <- option "" $ string "-" <|> string "+"
		num <- many1 digit
		let epart = 'e':(esign ++ num)
		return epart
	let num = read (sign ++ int ++ dec ++ pow) :: Double
	return num	
\end{lstlisting}
This parser parses out a double-precision floating point number, given a string which may or may not be in scientific notation (eg. 2.35 vs 3.31045e-5). Once we are convinced about the correctness of this parser, perhaps by throwing test cases at it, we can use this parser to implement a parser for bias vectors:
\begin{lstlisting}
vector :: Parser [Double]
vector = do
	void $ char '['
	eatWhitespace
	numbers <- sepEndBy number (many1 whitespace)
	void (char ']' <?> "vector closing bracket")
	return numbers
\end{lstlisting}
Here, we have a parser which returns a list of doubles, essentially a vector. As we convince ourselves of its correctness through testing, it occurs to us that this parser can apply not only to bias vectors, but also to the vectors within each matrix. After all, matrices are represented as lists of vectors. Then, we arrive at a parser for matrices:
\begin{lstlisting}
matrix :: Parser (Matrix Double, Int, Int)
matrix = do
	void $ char '['
	eatWhitespace
	rows <- sepEndBy vector (many1 whitespace)
	void (char ']' <?> "matrix closing bracket")
	let nRows = length rows
	let nCols = length $ head rows
	return (Mat.fromList nRows nCols (concat rows), nRows, nCols)
\end{lstlisting}
And so on we go, composing our parsers all the way up the chain until we have a fully functional model file parser. It is hard to overstate the convenience and confidence this approach of writing a parser provides, where we can incrementally test each stage of our parser, using it as a foundation for the next layer.

\subsubsection{Challenges \& Mitigation}
\subsubsection{Batch inference}
As a prerequisite for this section, here's what the network data structure looks like:
\begin{lstlisting}
type Network = [Layer]

data Layer = Layer {
		inDim :: Int,
		outDim :: Int,
		weights :: Matrix Double,
		biases :: Matrix Double
	}
\end{lstlisting}
Quite simply, a feed-forward neural network can be modeled as a list of layers. Now, to perform inference of a single vector on this network, we have:
\begin{lstlisting}
inferSingle :: Network -> Vector Double -> Vector Double
inferSingle net input = getCol 1 $ softLayer 
			(foldl reluLayer inpMat nonLinear) final
	where
		inpMat = colVector input
		size = length net
		nonLinear = take (size - 1) net
		final = last net

		inferLayer :: Matrix Double -> Layer -> Matrix Double
		inferLayer inp (Layer _ _ w b) = w * inp + b

		reluLayer :: Matrix Double -> Layer -> Matrix Double
		reluLayer inp lay = relu (inferLayer inp lay)

		softLayer :: Matrix Double -> Layer -> Matrix Double
		softLayer inp lay = softmax (inferLayer inp lay)
\end{lstlisting}
Assuming that \verb|relu| \& \verb|softmax| are easy to define (they are), this is a fairly simple construction. So, then, since the inferLayer function is already using matrices under the hood, and matrix multiplication distributes under column concatenation, surely batch inference is a simple extension, like so:
\begin{lstlisting}
inferBatch :: Network -> Matrix Double -> Matrix Double
inferBatch net input = softLayer (foldl reluLayer input nonLinear) final
	where
		size = length net
		nonLinear = take (size - 1) net
		final = last net

		inferLayer :: Matrix Double -> Layer -> Matrix Double
		inferLayer inp (Layer _ _ w b) = w * inp + b
		...
\end{lstlisting}
While this successfully compiles, this leads to quite an obtuse runtime error during batch inference.\medskip

After some exploration, it becomes clear that this is due to a mismatch in dimension of $Wx$ and $b$, since $b$ is a single column vector. Then, we just write a function to expand $b$ to the same column length as the input vector, by appending it to itself (column-wise):
\begin{lstlisting}
inferBatch :: Network -> Matrix Double -> Matrix Double
inferBatch net input = softLayer (foldl reluLayer input nonLinear) final
	where
		size = length net
		nonLinear = take (size - 1) net
		final = last net

		inputCols = ncols input

        expandCols :: Matrix Double -> Matrix Double
        expandCols mat = foldr (\_ acc -> acc <|> mat) mat [1..inputCols]

		inferLayer :: Matrix Double -> Layer -> Matrix Double
		inferLayer inp (Layer _ _ w b) = w * inp + (expandCols b)
		...
\end{lstlisting}
And this does solve the error, opening the gates for another, more subtle error. The batch inference is producing results, but the output activations don't look much like probabilities, as they should after a softmax.\bigskip

Upon investigating the softmax implementation, the cause becomes clear:
\begin{lstlisting}
softmax :: Matrix Double -> Matrix Double
softmax v = (/ denom) . exp <$> v
			where denom = sum (exp <$> v)	
\end{lstlisting}
This works perfectly fine when there is exactly one column in the matrix. When this is not the case, however, the denominator is accumulated over all elements of the matrix, instead of per output vector. Therefore, we need to generalise this single column softmax function to a multi column soft max function.\bigskip

The cleanest solution I have been able to come up for this at this time is:
\begin{lstlisting}
batchSoftmax :: Matrix Double -> Matrix Double
batchSoftmax mat = submatrix 1 rows 1 inputCols (foldr folder (fromList rows 1 [1..]) [1..inputCols])
	where
		rows = nrows mat
		folder :: Int -> Matrix Double -> Matrix Double
		folder colIdx acc = colVector ((/ denom col) . exp <$> col) <|> acc
			where col = getCol colIdx mat

		denom :: Vector Double -> Double
		denom col = sum (exp <$> col)
\end{lstlisting}

\section{Results \& Discussion}
\subsection{Example Runs}
{
	\begin{center}
		\includegraphics[width=\textwidth]{../images/examples.png}
	\end{center}
	\begin{center}
		Interactive Mode
	\end{center}
	\begin{center}
		\includegraphics[width=\textwidth]{../images/examplebatch.png}
	\end{center}
	\begin{center}
		Batched Mode
	\end{center}
}

\subsection{Testing Outcomes}

\subsection{Profiling Outcomes}

\subsection{Limitations}
\begin{itemize}
	\item Lack of runtime checks for user input, leading to a restrictive input format \& several non-graceful exits, without any useful feedback to the user.
	\item Class of models that works limited to ReLU + Softmax activation based classifiers (large but not comprehensive).
	\item The csv input in batch mode can't end on a blank line (parser bug)
	\item Batch mode doesn't provide summary statistics
	
	\item Functionality is limited to feed-forward classifiers, though this is purely due to presentation of the output
	\item The command-line interface, while designed for usability, still may serve as a barrier for the most technically-challenged individuals
	\item Requires the weights to be in a well-known format, and for the user to locate the weights file manually
\end{itemize}

\section{Conclusions}
A high-level roadmap for the project's completion is as follows:
\begin{itemize}
	\item Perform runtime checks on input dimension during inference.
	\item Runtime checks for interactive mode (for graceful fails).
	\item Implement the test suite discussed above.
	\item Transition to a binary data format for the model files.
	\item Allow alternative activation functions for non-output layers, as opposed to assuming ReLU.
	\item Add testing mode, given batch and expected outputs, report model accuracy.
	\item Add aggregation mode, use multiple trained models with voting (ensemble method).
\end{itemize}
Upon the completion of all these tasks, the project will be in its completed state.\bigskip

The choice to develop tests \& more robust runtime checks for the existing features before moving onto the new features is so that I can gain a better understanding of how to structure exception handling in haskell, and also learn how to use HSpec for unit tests. Then, as the new features are being implemented, a test-driven approach can be followed, allowing the new features to be verified as they're implemented.

\section{Extensions \& Future Work}
\begin{itemize}
	\item Infer the format of the weights file based on a few well-known formats (.pkl, .pth, .safetensors), instead of only working for safetensors. This requires more than just looking at the extension, as we have to further infer where the weights and biases of each layer lie based on the keys in the file. 
\end{itemize}

\section{References}

\end{document}

